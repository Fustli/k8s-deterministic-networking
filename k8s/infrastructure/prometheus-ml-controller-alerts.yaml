apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: ml-controller-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus
    role: alert-rules
spec:
  groups:
  - name: ml-controller.rules
    interval: 30s
    rules:
    
    # === CRITICAL ALERTS ===
    
    - alert: MLControllerNoLeader
      expr: sum(ml_controller_is_leader) == 0
      for: 30s
      labels:
        severity: critical
        component: ml-controller
        team: networking
      annotations:
        summary: "ML Controller has no active leader"
        description: "No ML Controller instance is currently acting as leader. Bandwidth control is stopped."
        runbook_url: "https://runbooks.company.com/ml-controller/no-leader"
        action: "Check controller pods and leader election lease status"
    
    - alert: MLControllerAllPodsDown
      expr: up{job="ml-controller-ha-metrics"} == 0
      for: 60s
      labels:
        severity: critical
        component: ml-controller
        team: networking
      annotations:
        summary: "All ML Controller pods are down"
        description: "All ML Controller HA instances are unreachable. System has no bandwidth control."
        runbook_url: "https://runbooks.company.com/ml-controller/all-pods-down"
        action: "Immediate investigation required - check deployment, nodes, and cluster health"
    
    - alert: MLControllerLeaderFlapping
      expr: changes(ml_controller_is_leader[10m]) > 5
      for: 2m
      labels:
        severity: critical
        component: ml-controller
        team: networking
      annotations:
        summary: "ML Controller leadership is flapping"
        description: "Leadership has changed {{ $value }} times in the last 10 minutes, indicating instability."
        runbook_url: "https://runbooks.company.com/ml-controller/leader-flapping"
        action: "Check network connectivity, API server health, and pod stability"
    
    # === HIGH PRIORITY ALERTS ===
    
    - alert: MLControllerHighMemoryUsage
      expr: container_memory_working_set_bytes{pod=~"ml-controller-ha-.*"} / container_spec_memory_limit_bytes > 0.8
      for: 5m
      labels:
        severity: high
        component: ml-controller
        team: networking
      annotations:
        summary: "ML Controller pod {{ $labels.pod }} high memory usage"
        description: "Pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} of its memory limit."
        runbook_url: "https://runbooks.company.com/ml-controller/high-memory"
        action: "Monitor for memory leaks, consider increasing limits"
    
    - alert: MLControllerHighCPUUsage
      expr: rate(container_cpu_usage_seconds_total{pod=~"ml-controller-ha-.*"}[5m]) > 0.8
      for: 10m
      labels:
        severity: high
        component: ml-controller
        team: networking
      annotations:
        summary: "ML Controller pod {{ $labels.pod }} high CPU usage"
        description: "Pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} CPU for 10+ minutes."
        runbook_url: "https://runbooks.company.com/ml-controller/high-cpu"
        action: "Check for performance issues, consider CPU limit adjustment"
    
    - alert: MLControllerPodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total{pod=~"ml-controller-ha-.*"}[15m]) * 60 * 15 > 5
      for: 5m
      labels:
        severity: high
        component: ml-controller
        team: networking
      annotations:
        summary: "ML Controller pod {{ $labels.pod }} is crash looping"
        description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last 15 minutes."
        runbook_url: "https://runbooks.company.com/ml-controller/crash-loop"
        action: "Check pod logs for errors, investigate resource issues or configuration problems"
    
    - alert: MLControllerMetricsUnhealthy
      expr: ml_controller_metrics_healthy{ml_controller_is_leader="1"} == 0
      for: 3m
      labels:
        severity: high
        component: ml-controller
        team: networking
      annotations:
        summary: "ML Controller leader cannot reach Prometheus metrics"
        description: "The active leader {{ $labels.instance_id }} cannot connect to Prometheus for {{ $value }} minutes."
        runbook_url: "https://runbooks.company.com/ml-controller/metrics-unhealthy"
        action: "Check Prometheus connectivity, network policies, and service endpoints"
    
    # === MEDIUM PRIORITY ALERTS ===
    
    - alert: MLControllerBandwidthStuck
      expr: |
        (
          ml_controller_current_bandwidth_mbps{ml_controller_is_leader="1"} 
          and on (instance_id) 
          (ml_controller_current_bandwidth_mbps{ml_controller_is_leader="1"} offset 5m)
        ) == 1
      for: 10m
      labels:
        severity: medium
        component: ml-controller
        team: networking
      annotations:
        summary: "ML Controller bandwidth has not changed for 10+ minutes"
        description: "Bandwidth has been stuck at {{ $value }}Mbps for over 10 minutes, may indicate control loop issues."
        runbook_url: "https://runbooks.company.com/ml-controller/bandwidth-stuck"
        action: "Check jitter metrics, control loop logs, and system responsiveness"
    
    - alert: MLControllerLowReplicas
      expr: kube_deployment_status_replicas_available{deployment="ml-controller-ha"} < 2
      for: 5m
      labels:
        severity: medium
        component: ml-controller
        team: networking
      annotations:
        summary: "ML Controller has insufficient replicas for HA"
        description: "Only {{ $value }} replicas available, HA requires at least 2 for failover capability."
        runbook_url: "https://runbooks.company.com/ml-controller/low-replicas"
        action: "Check pod status, node resources, and deployment scaling"
    
    - alert: MLControllerBandwidthAtMinimum
      expr: ml_controller_current_bandwidth_mbps{ml_controller_is_leader="1"} <= 10
      for: 15m
      labels:
        severity: medium
        component: ml-controller
        team: networking
      annotations:
        summary: "ML Controller bandwidth at minimum setting"
        description: "Bandwidth has been at minimum ({{ $value }}Mbps) for 15+ minutes, indicating high network jitter."
        runbook_url: "https://runbooks.company.com/ml-controller/bandwidth-minimum"
        action: "Investigate network performance issues and jitter sources"
    
    - alert: MLControllerBandwidthAtMaximum
      expr: ml_controller_current_bandwidth_mbps{ml_controller_is_leader="1"} >= 1000
      for: 30m
      labels:
        severity: medium
        component: ml-controller
        team: networking
      annotations:
        summary: "ML Controller bandwidth at maximum setting"
        description: "Bandwidth has been at maximum ({{ $value }}Mbps) for 30+ minutes."
        runbook_url: "https://runbooks.company.com/ml-controller/bandwidth-maximum"
        action: "Consider increasing maximum bandwidth limit if network can handle more"
    
    # === LOW PRIORITY / INFO ALERTS ===
    
    - alert: MLControllerNewLeaderElected
      expr: changes(ml_controller_is_leader[5m]) > 0 and ml_controller_is_leader == 1
      for: 0m
      labels:
        severity: info
        component: ml-controller
        team: networking
      annotations:
        summary: "New ML Controller leader elected"
        description: "Instance {{ $labels.instance_id }} has become the new leader."
        runbook_url: "https://runbooks.company.com/ml-controller/leader-change"
        action: "Monitor for stable operation, verify control loop continuation"
    
    - alert: MLControllerPodRestarted
      expr: time() - kube_pod_start_time{pod=~"ml-controller-ha-.*"} < 300
      for: 1m
      labels:
        severity: info
        component: ml-controller
        team: networking
      annotations:
        summary: "ML Controller pod {{ $labels.pod }} recently restarted"
        description: "Pod {{ $labels.pod }} started {{ $value | humanizeDuration }} ago."
        runbook_url: "https://runbooks.company.com/ml-controller/pod-restart"
        action: "Monitor for successful initialization and leader election participation"
    
    # === RECORDING RULES ===
    
    - record: ml_controller:leader_uptime_seconds
      expr: |
        (time() - kube_pod_start_time{pod=~"ml-controller-ha-.*"}) 
        * on (pod) group_right() 
        (ml_controller_is_leader{ml_controller_is_leader="1"} > 0)
    
    - record: ml_controller:bandwidth_change_rate
      expr: |
        abs(
          ml_controller_current_bandwidth_mbps{ml_controller_is_leader="1"} 
          - ml_controller_current_bandwidth_mbps{ml_controller_is_leader="1"} offset 5m
        ) / 5
    
    - record: ml_controller:replica_health_percentage
      expr: |
        (
          count(ml_controller_control_loop_running == 1) / 
          count(ml_controller_control_loop_running)
        ) * 100
    
    - record: ml_controller:leader_election_frequency
      expr: |
        rate(changes(ml_controller_is_leader[1h])[1h:5m])