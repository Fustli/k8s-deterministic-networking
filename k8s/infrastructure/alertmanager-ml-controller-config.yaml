apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-notification-config
  namespace: monitoring
type: Opaque
stringData:
  # Slack webhook URL - replace with actual webhook
  slack_webhook_url: "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
  
  # PagerDuty integration key - replace with actual key
  pagerduty_routing_key: "YOUR_PAGERDUTY_INTEGRATION_KEY"
  
  # Email SMTP configuration
  smtp_smarthost: "smtp.company.com:587"
  smtp_from: "alerts@company.com"
  smtp_auth_username: "alerts@company.com"
  smtp_auth_password: "YOUR_SMTP_PASSWORD"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-ml-controller-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'smtp.company.com:587'
      smtp_from: 'alerts@company.com'
      smtp_auth_username: 'alerts@company.com'
      smtp_auth_password: '${SMTP_PASSWORD}'
      
      # PagerDuty global configuration
      pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'
      
      # Slack global configuration  
      slack_api_url: '${SLACK_WEBHOOK_URL}'
    
    # Route tree for ML Controller alerts
    route:
      group_by: ['alertname', 'component', 'severity']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h
      receiver: 'default-receiver'
      
      routes:
      # Critical alerts - immediate notification via PagerDuty + Slack
      - match:
          component: ml-controller
          severity: critical
        receiver: 'critical-ml-controller'
        group_wait: 0s
        repeat_interval: 30m
        continue: true
      
      # High priority alerts - Slack notification  
      - match:
          component: ml-controller
          severity: high
        receiver: 'high-ml-controller'
        group_wait: 30s
        repeat_interval: 2h
        continue: true
      
      # Medium priority alerts - Slack notification during business hours
      - match:
          component: ml-controller
          severity: medium
        receiver: 'medium-ml-controller'
        group_wait: 5m
        repeat_interval: 8h
      
      # Info alerts - logging only
      - match:
          component: ml-controller
          severity: info
        receiver: 'info-ml-controller'
        group_wait: 10m
        repeat_interval: 24h
    
    receivers:
    
    # Default receiver (fallback)
    - name: 'default-receiver'
      email_configs:
      - to: 'ops-team@company.com'
        subject: 'ALERT: {{ .GroupLabels.alertname }} - {{ .GroupLabels.component }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Severity: {{ .Labels.severity }}
          {{ end }}
    
    # Critical ML Controller alerts
    - name: 'critical-ml-controller'
      # PagerDuty for immediate response
      pagerduty_configs:
      - routing_key: '${PAGERDUTY_ROUTING_KEY}'
        description: 'CRITICAL: {{ .GroupLabels.alertname }}'
        details:
          summary: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
          source: 'ML Controller Monitoring'
          severity: '{{ .CommonLabels.severity }}'
          component: '{{ .CommonLabels.component }}'
          team: '{{ .CommonLabels.team }}'
        links:
        - href: 'https://grafana.company.com/d/ml-controller'
          text: 'ML Controller Dashboard'
        - href: '{{ range .Alerts }}{{ .Annotations.runbook_url }}{{ end }}'
          text: 'Runbook'
      
      # Slack for team awareness  
      slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#alerts-critical'
        username: 'ML Controller Monitor'
        icon_emoji: ':rotating_light:'
        title: 'CRITICAL ML Controller Alert'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Action:* {{ .Annotations.action }}
          *Runbook:* {{ .Annotations.runbook_url }}
          {{ end }}
        actions:
        - type: button
          text: 'View Dashboard'
          url: 'https://grafana.company.com/d/ml-controller'
        - type: button
          text: 'View Logs'
          url: 'https://logs.company.com/ml-controller'
    
    # High priority ML Controller alerts
    - name: 'high-ml-controller'
      slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#alerts-networking'
        username: 'ML Controller Monitor'
        icon_emoji: ':warning:'
        title: 'HIGH Priority ML Controller Alert'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Severity:* {{ .Labels.severity }}
          *Action:* {{ .Annotations.action }}
          {{ end }}
        actions:
        - type: button
          text: 'View Dashboard'
          url: 'https://grafana.company.com/d/ml-controller'
    
    # Medium priority ML Controller alerts  
    - name: 'medium-ml-controller'
      slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#alerts-networking'
        username: 'ML Controller Monitor' 
        icon_emoji: ':information_source:'
        title: 'ℹ️ Medium Priority ML Controller Alert'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          {{ end }}
    
    # Info ML Controller alerts
    - name: 'info-ml-controller'
      slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#ml-controller-info'
        username: 'ML Controller Monitor'
        icon_emoji: ':information_source:'
        title: 'ML Controller Information'
        text: |
          {{ range .Alerts }}
          *Info:* {{ .Annotations.summary }}
          *Details:* {{ .Annotations.description }}
          {{ end }}
    
    # Inhibition rules - suppress lower priority alerts when higher priority fires
    inhibit_rules:
    - source_match:
        component: ml-controller
        severity: critical
      target_match:
        component: ml-controller
      target_match_re:
        severity: high|medium|info
      equal: ['instance_id', 'pod']
    
    - source_match:
        component: ml-controller  
        severity: high
      target_match:
        component: ml-controller
      target_match_re:
        severity: medium|info
      equal: ['instance_id', 'pod']
    
    # Templates for reusable message formatting
    templates:
    - '/etc/alertmanager/templates/*.tmpl'
  
  # Custom message templates
  ml_controller_templates.tmpl: |
    {{ define "ml_controller.title" }}
    [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] 
    ML Controller {{ .GroupLabels.alertname }}
    {{ end }}
    
    {{ define "ml_controller.description" }}
    {{ range .Alerts }}
    {{ if .Annotations.summary }}{{ .Annotations.summary }}{{ end }}
    {{ if .Annotations.description }}{{ .Annotations.description }}{{ end }}
    {{ end }}
    {{ end }}
    
    {{ define "ml_controller.runbook" }}
    {{ range .Alerts }}
    {{ if .Annotations.runbook_url }}
    Runbook: {{ .Annotations.runbook_url }}
    {{ end }}
    {{ end }}
    {{ end }}